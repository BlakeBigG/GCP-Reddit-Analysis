{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from google.cloud import storage, language_v1\n",
    "import getpass\n",
    "from sqlalchemy.engine.url import URL\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "# Define values of project, bucket_name, and url\n",
    "PROJECT='umc-dsa-8420-fs2021'\n",
    "bucket_name = 'bmgwd9-final-bucket'\n",
    "storage_client = storage.Client(project=PROJECT)\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "\n",
    "\n",
    "    \n",
    "def sample_analyze_sentiment(text_content):\n",
    "    \"\"\"\n",
    "    Analyzing Sentiment in a String\n",
    "\n",
    "    Args:\n",
    "      text_content The text content to analyze\n",
    "    \"\"\"\n",
    "\n",
    "    client = language_v1.LanguageServiceClient()\n",
    "\n",
    "    # Available types: PLAIN_TEXT, HTML\n",
    "    type_ = language_v1.Document.Type.PLAIN_TEXT\n",
    "\n",
    "    # Optional. If not specified, the language is automatically detected.\n",
    "    # For list of supported languages:\n",
    "    # https://cloud.google.com/natural-language/docs/languages\n",
    "    language = \"en\" # Must use or will get errors regarding unsupported languages\n",
    "    document = {\"content\": text_content, \"type_\": type_, \"language\": language}\n",
    "\n",
    "    # Available values: NONE, UTF8, UTF16, UTF32\n",
    "    encoding_type = language_v1.EncodingType.UTF8\n",
    "\n",
    "    response = client.analyze_sentiment(request = {'document': document, 'encoding_type': encoding_type})\n",
    "    \n",
    "    output = {'sentiment_score': response.document_sentiment.score, 'sentiment_magnitude': response.document_sentiment.magnitude}\n",
    "    return output\n",
    "\n",
    "master_df = pd.DataFrame()\n",
    "num_processed = 0\n",
    "for blob in bucket.list_blobs():\n",
    "    post_dict = json.loads(blob.download_as_string(client=None))\n",
    "    # Initialize a new dictionary to contain previous information and sentiment scores\n",
    "    my_dict = {\"title\":[], \"summary\":[], \"timestamp\": [], \"link\":[], \"title_sentiment_score\":[],\n",
    "                     \"title_sentiment_magnitude\":[], \"summary_sentiment_score\":[], \"summary_sentiment_magnitude\":[]}\n",
    "    \n",
    "    for i in range(len(post_dict['title'])):\n",
    "        incompatible_timestamp = post_dict['timestamp'][i]\n",
    "        compatible_timestamp = incompatible_timestamp.replace(\"T\", \" \")\n",
    "        my_dict[\"timestamp\"].append(compatible_timestamp)\n",
    "        my_dict[\"title\"].append(post_dict['title'][i])\n",
    "        my_dict[\"summary\"].append(post_dict['summary'][i])\n",
    "        my_dict[\"link\"].append(post_dict['link'][i])\n",
    "        \n",
    "        title = post_dict['title'][i] # get title[i]\n",
    "        title_nl_output = sample_analyze_sentiment(title)\n",
    "        print(title_nl_output)\n",
    "        my_dict[\"title_sentiment_score\"].append(title_nl_output[\"sentiment_score\"])\n",
    "        my_dict[\"title_sentiment_magnitude\"].append(title_nl_output[\"sentiment_magnitude\"])\n",
    "        \n",
    "        summary = post_dict['summary'][i]\n",
    "        summary_nl_output = sample_analyze_sentiment(summary)\n",
    "        print(summary_nl_output)\n",
    "        my_dict[\"summary_sentiment_score\"].append(summary_nl_output[\"sentiment_score\"])\n",
    "        my_dict[\"summary_sentiment_magnitude\"].append(summary_nl_output[\"sentiment_magnitude\"])\n",
    "        \n",
    "        \n",
    "    # convert processed dictionary to dataframe\n",
    "    df = pd.DataFrame.from_dict(my_dict)\n",
    "    \n",
    "    master_df = pd.concat([master_df, df])\n",
    "    \n",
    "    num_processed += 1 # Counts the number of json objects processed\n",
    "\n",
    "    # Limit the number of api calls to save money\n",
    "    if num_processed >= 200:\n",
    "        break    \n",
    "    \n",
    "    # Wait 10 seconds after procesing each json to avoid api call cap\n",
    "    time.sleep(10)\n",
    "\n",
    "print(master_df.shape)\n",
    "master_df.to_csv('test_output.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When you are ready, let's export this\n",
    " 1. Open terminal\n",
    " 2. Change into course, module6, practices folder\n",
    " 3. Convert to a script: `jupyter-nbconvert --to script StorageTestCode.ipynb`\n",
    "```\n",
    "[NbConvertApp] Converting notebook StorageTestCode.ipynb to script\n",
    "[NbConvertApp] Writing 4740 bytes to StorageTestCode.py\n",
    "```\n",
    "\n",
    "#### Move the generated script using `scp` or copy and paste techniques to get it to your VM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
